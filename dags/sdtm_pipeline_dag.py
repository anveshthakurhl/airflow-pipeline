import os
from datetime import datetime

import shutil
import pandas as pd
from airflow.decorators import task
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.sdk import dag

from loaders.DMLoader import DMLoader

# --- Configuration ---
S3_BUCKET_NAME = "headlamp-beacon-test"
S3_PREFIX = "STUDY001/"  
LOCAL_BASE_DOWNLOAD_DIR = "./tmp/s3_downloads/"

AWS_CONN_ID = "aws_default"
SDTM_DOMAIN_COLUMNS = ["ae", "dm", "se"]
POSTGRES_TABLE_NAME = "test_table"  # Adjust this to your actual table name

LOADER_REGISTRY = {"dm": DMLoader}

@task
def clean_local_download_directory(base_download_dir: str):
    """
    Deletes the specified local download directory if it exists,
    ensuring a clean state for new downloads.
    """
    print(f"Checking for and cleaning directory: {base_download_dir}")
    if os.path.exists(base_download_dir) and os.path.isdir(base_download_dir):
        try:
            shutil.rmtree(base_download_dir)
            print(f"Successfully deleted existing directory: {base_download_dir}")
        except Exception as e:
            print(f"Error deleting directory {base_download_dir}: {e}")
            raise # Re-raise to fail the task if cleanup fails
    else:
        print(f"Directory {base_download_dir} does not exist, no cleanup needed.")
        
@task
def list_s3_files(
    bucket_name: str, prefix: str = "", aws_conn_id: str = "aws_default"
) -> list[str]:
    s3_hook = S3Hook(aws_conn_id=aws_conn_id)
    print(f"Listing files in s3://{bucket_name}/{prefix}...")
    keys = s3_hook.list_keys(bucket_name=bucket_name, prefix=prefix)

    file_keys = [key for key in keys if not key.endswith("/")]

    if file_keys:
        print(f"Found {len(keys)} files in s3://{bucket_name}/{prefix}:")
        for key in keys:
            print(f"- {key}")
    else:
        print(f"No files found in s3://{bucket_name}/{prefix}.")
        file_keys = []
    return file_keys


@task
def download_single_s3_file(
    s3_key: str,
    bucket_name: str,
    base_download_dir: str,
    aws_conn_id: str = "aws_default",
):
    s3_hook = S3Hook(aws_conn_id=aws_conn_id)

    full_local_file_path = os.path.join(base_download_dir, s3_key)

    local_directory_for_download = os.path.dirname(full_local_file_path)

    os.makedirs(local_directory_for_download, exist_ok=True)

    print(
        f"Attempting to download s3://{bucket_name}/{s3_key} to directory {local_directory_for_download} (final file: {full_local_file_path})"
    )

    try:
        s3_hook.download_file(
            key=s3_key,
            bucket_name=bucket_name,
            local_path=local_directory_for_download,
            preserve_file_name=True,
            use_autogenerated_subdir=False,
        )

        final_downloaded_name = os.path.basename(s3_key)
        verified_file_path = os.path.join(
            local_directory_for_download, final_downloaded_name
        )

        print(f"Successfully downloaded {s3_key} to {verified_file_path}")
        return verified_file_path

    except Exception as e:
        print(f"Error downloading file {s3_key} from S3: {e}")
        raise


@task
def load_csv_to_db(
    s3_key: str,
    local_file_path: str,
    table_name: str,
    postgres_conn_id: str = "postgres_default",
):
    if not os.path.exists(local_file_path):
        raise FileNotFoundError(f"CSV file not found at: {local_file_path}")

    print(f"Processing CSV '{local_file_path}' (S3 Key: {s3_key}) for DB load...")

    try:
        df = pd.read_csv(local_file_path)

        # The 'USUBJID' column will now be directly identified by its name from the header.
        if "USUBJID" not in df.columns:
            raise ValueError(
                f"Column 'USUBJID' not found in file '{local_file_path}'. "
                f"Please ensure the CSV has a header row with a column named 'USUBJID'."
            )

        if "DOMAIN" not in df.columns:
            raise ValueError(
                f"Required column 'DOMAIN' was not found in file '{local_file_path}'."
            )

        print(f"{df.head()}")

        domain_name = df["DOMAIN"].iloc[0].lower()
        print(f"Detected domain name: {domain_name}")

        if domain_name not in SDTM_DOMAIN_COLUMNS:
            print(
                f"WARNING: Derived domain '{domain_name}' not in expected columns {SDTM_DOMAIN_COLUMNS}. Skipping file: {s3_key}"
            )
            return

        loader_class = LOADER_REGISTRY.get(domain_name)
        if not loader_class:
            print(
                f"No loader found for domain '{domain_name}'. Skipping file: {s3_key}"
            )
            return

        pg_hook = PostgresHook(postgres_conn_id=postgres_conn_id)
        conn = pg_hook.get_conn()
        loader = loader_class()
        loader.load_data(df, conn)
    except Exception as e:
        print(f"Error loading CSV file {local_file_path} to PostgreSQL: {e}")
        if "conn" in locals() and conn:
            conn.rollback()
        raise
    finally:
        if "conn" in locals() and conn:
            conn.close()


@dag(
    dag_id="sdtm_pipeline_dag",
    start_date=datetime(2023, 1, 1),
    schedule=None,
    catchup=False,
    tags=["s3", "sdtm", "postgres", "beacon"],
    doc_md="""
    This DAG demonstrates how to dynamically list and download all files
    from a specified S3 bucket (or a prefix within it) using Airflow's
    TaskFlow API and dynamic task mapping. and then upload the 
    SDTM data from it to a PostgreSQL database.
    """,
)
def s3_download_all_workflow():
    
    cleanup_task = clean_local_download_directory(base_download_dir=LOCAL_BASE_DOWNLOAD_DIR)
    
    s3_keys_to_download = list_s3_files(
        bucket_name=S3_BUCKET_NAME, prefix=S3_PREFIX, aws_conn_id=AWS_CONN_ID
    )

    downloaded_file_info = download_single_s3_file.partial(
        bucket_name=S3_BUCKET_NAME,
        base_download_dir=LOCAL_BASE_DOWNLOAD_DIR,
        aws_conn_id=AWS_CONN_ID,
    ).expand(s3_key=s3_keys_to_download)

    load_to_db_task = load_csv_to_db.partial(
        table_name=POSTGRES_TABLE_NAME, postgres_conn_id="postgres_default"
    ).expand(s3_key=s3_keys_to_download, local_file_path=downloaded_file_info)

    cleanup_task >> s3_keys_to_download >> downloaded_file_info >> load_to_db_task

_ = s3_download_all_workflow()
