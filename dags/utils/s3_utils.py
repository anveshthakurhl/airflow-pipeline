from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.decorators import task
from airflow.sdk import get_current_context
import os
import shutil

@task
def clean_local_download_directory(base_download_dir: str):
    if os.path.exists(base_download_dir) and os.path.isdir(base_download_dir):
        shutil.rmtree(base_download_dir)

@task
def list_s3_files(aws_conn_id: str = "aws_default") -> list[str]:
    context = get_current_context()
    dag_run_conf = context.get('dag_run').conf if context.get('dag_run') else {}

    bucket_name = dag_run_conf.get('s3_bucket_name')
    prefix = dag_run_conf.get('s3_prefix')
    s3_hook = S3Hook(aws_conn_id=aws_conn_id)
    keys = s3_hook.list_keys(bucket_name=bucket_name, prefix=prefix)
    return [key for key in keys if not key.endswith("/")]

@task
def download_single_s3_file(s3_key: str, base_download_dir: str, aws_conn_id: str = "aws_default"):
    context = get_current_context()
    dag_run_conf = context.get('dag_run').conf if context.get('dag_run') else {}

    bucket_name = dag_run_conf.get('s3_bucket_name')
    s3_hook = S3Hook(aws_conn_id=aws_conn_id)
    full_local_file_path = os.path.join(base_download_dir, s3_key)
    local_directory_for_download = os.path.dirname(full_local_file_path)
    os.makedirs(local_directory_for_download, exist_ok=True)
    s3_hook.download_file(
        key=s3_key,
        bucket_name=bucket_name,
        local_path=local_directory_for_download,
        preserve_file_name=True,
        use_autogenerated_subdir=False,
    )
    return os.path.join(local_directory_for_download, os.path.basename(s3_key))
